% !TeX spellcheck = en_US
\documentclass[10pt,a4paper,twocolumn,twoside]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{times}
\usepackage{titlesec}
\usepackage{multirow}
\usepackage{lettrine}
\usepackage{pdflscape}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage[table,xcdraw]{xcolor}
\usepackage[edges]{forest}
\usepackage{url}
\usepackage[top=2cm, bottom=1.5cm, left=2cm, right=2cm]{geometry}
\usepackage[figurename=Fig.,tablename=Table]{caption}

\author{\LARGE\sffamily Segovia Barreales, Richard}
\title{\Huge{\sffamily Reducing dizziness when using a video-see-through head-mounted display}}
\date{}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

%
%\large\bfseries\sffamily
\titleformat{\section}
{\large\sffamily\scshape\bfseries}
{\textbf{\thesection}}{1em}{}

\begin{document}

	\fancyhead[LO]{\scriptsize AUTHOR: SEGOVIA BARREALES, RICHARD}
	\fancyhead[RO]{\thepage}
	\fancyhead[LE]{\thepage}
	\fancyhead[RE]{\scriptsize EE/UAB TFG INFORMÀTICA: REDUCING DIZZINESS WHEN USING A VIDEO-SEE-THROUGH HEAD-MOUNTED DISPLAY}
	
	\fancyfoot[CO,CE]{}
	
	\fancypagestyle{primerapagina}
	{
	   \fancyhf{}
	   \fancyhead[L]{\scriptsize TFG EN ENGINYERIA INFORMÀTICA, ESCOLA D'ENGINYERIA (EE), UNIVERSITAT AUTÒNOMA DE BARCELONA (UAB)}
	   \fancyfoot[C]{\scriptsize June 2018, Escola d'Enginyeria (UAB)}
	}
	
	%\lhead{\thepage}
	%\chead{}
	%\rhead{\tiny EE/UAB TFG INFORMÀTICA: TÍTOL (ABREUJAT SI ÉS MOLT LLARG)}
	%\lhead{ EE/UAB \thepage}
	%\lfoot{}
	%\cfoot{\tiny{February 2015, Escola d'Enginyeria (UAB)}}
	%\rfoot{}
	\renewcommand{\headrulewidth}{0pt}
	\renewcommand{\footrulewidth}{0pt}
	\pagestyle{fancy}
	
	%\thispagestyle{myheadings}
	\twocolumn[\begin{@twocolumnfalse}
	
	%\vspace*{-1cm}{\scriptsize TFG EN ENGINYERIA INFORMÀTICA, ESCOLA D'ENGINYERIA (EE), UNIVERSITAT AUTÒNOMA DE BARCELONA (UAB)}
	
	\maketitle
	
	\thispagestyle{primerapagina}
	%\twocolumn[\begin{@twocolumnfalse}
	%\maketitle
	%\begin{abstract}
	\begin{center}
	\parbox{0.915\textwidth}
	{\sffamily
	\textbf{Resum--}
	Resum del projecte, màxim 10 línies. ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... .................. ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........ .......... ..  ... . ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ........... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... 
	\\
	\\
	\textbf{Paraules clau-- } Acomodació i vergència , mapes de profunditat, calibratge estèreo.\\
	\\
	%\end{abstract}
	%\bigskip
	%\begin{abstract}
	\bigskip
	\\
	\textbf{Abstract--} Versió en anglès del resum . ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... .................. ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........ .......... ..  ... . ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ........... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... ........ ........... .......... ..  ... ..... .... 
	\\
	\\
	\textbf{Keywords-- } Accomodation vergence conflict, video-see-through, head mounted display, depth map, stereo calibration. 
	}
	
	\bigskip
	
	{\vrule depth 0pt height 0.5pt width 4cm\hspace{7.5pt}%
	\raisebox{-3.5pt}{\fontfamily{pzd}\fontencoding{U}\fontseries{m}\fontshape{n}\fontsize{11}{12}\selectfont\char70}%
	\hspace{7.5pt}\vrule depth 0pt height 0.5pt width 4cm\relax}
	
	\end{center}
	
	\bigskip
	%\end{abstract}
	\end{@twocolumnfalse}]
	
	\blfootnote{$\bullet$ E-mail: richard.segovia@e-campus.uab.cat}
	\blfootnote{$\bullet$ Menció en Computació}
	\blfootnote{$\bullet$ Project supervised by: Coen Antens (CVC) and Felipe Lumbreras (Computació)}
	\blfootnote{$\bullet$ Course 2017/18}
	
	\section{Introduction}
	%- introduccion al vr
	%- explicar motivacion, practicas de verano, procarelight, usuarios mareados
	%- explicar projecto en si, idea
	%- explicar Accomodation vergence con un dibujo y de forma resumida
	
	The development of the technologies related with head mounted displays (HMD) has grown in the recent years mainly centered in the video-games field, some examples are the Oculus \cite{web:oculus} or the HTC Vive \cite{web:vive}. These devices are called virtual reality headsets because they are only capable of showing computer generated scenes.  
	
	Despite that the industry is mainly focused on developing virtual reality systems, there are two kinds of HMD that allow the visualization of the environment surrounding the user.  

	\begin{itemize}
	\item The optical-see-through devices use image projectors that display the image over a see-through mirror, hence allowing the user to see computer generated images over the environment. An example of this kind of devices is the Microsoft HoloLens \cite{web:hololens}.
	\item The video-see-through devices use one or two cameras placed in the front of the headset and show the stream of images in two screens placed in front of the eyes. This project uses these kind of devices, in Fig.\ref{fig:proto} one of the prototypes can be seen. 
	\end{itemize}

	\begin{figure}
		\centering
		\includegraphics[width=1\linewidth]{img/imagenproto3.jpg}
		\caption{Current version of the HMD video-see-through prototype.}
		\label{fig:proto}
	\end{figure}
	
	Both types of devices can show stereo images, this allow the user to sense depth in the displayed images. 
	As both types of devices are able to show different images for each eye, these systems are able to give the user stereo experiences. 
	
	This project originated from the need to establish and resolved the reasons why the users have a poor experience, dizziness and eye strain, when using video-see-through devices. As is explained in \cite{disconfortReview} and was experimentally tested in \cite{vergenceDisconfort}, the Accommodation-Vergence conflict is one of the issues that causes this poor user experience.
	
	The vergence is the process where the eyes set their angle of visualization trying to fuse the image of an object keeping it into sharp focus, whereas the accommodation is the process where the objects difficult to fuse are blurred. Both process are tightly coupled giving each other feedback in order to keep the images as sharp and fused as possible. 
	
	The issue arises as a result of using near eye screens to show stereo 3D. In these displays the image is shown always at the same distance of the eye, however, the distance between objects, disparity,  changes when the environment is changed. For example, changing from looking at a distant object to a close object.  This conflict can be seen in Fig.\ref{fig:vergence}.
	
	\begin{figure}
		\centering
		\includegraphics[width=1\linewidth]{img/vergencia.png}
		\caption{ In A and C can be seen the usual effect that happens when the coupling of the accommodation vergence is correct. As the accomodation distance is the same as the vergence distance, a blur effect appears in the corners. In B and D can be seen the effect that happens when an stereo scene is showed through near eye screens. As the accomodation distance is different than the vergence distance, no blur effect is applied in the corners. Source \cite{vergenceDisconfort}.}
		\label{fig:vergence}
	\end{figure}
	
	Parallel to the accommodation vergence conflict, other issue that happens when using video-see-thought is that objects seem to have sizes different than real. This is an issue when the user is trying to grab or manipulate objects and generally when the user is estimating distances. This issue happens because the camera is not in the same position as the eyes producing an error in the size perception.
	
	Consequently, this project tries to solve Accommodation-Vergence conflict and the size issue using environment information to set the distance between images of the viewer and change size settings.
	
		
	\section{State of the art}
	\label{sec:art}
	%explicar de donde viene el vr, hacia donde va, tecnicas tipicas usadas para resolucion del problema de la vergencia (seguimiento de los ojos), hablar de tecnicas de depth map
	%- hardware que se utiliza ahora, problema y incovenientes
	%- depth map, Libelas, explicar que existe la middlebury datebase y porque hemos elegido libelas???????????????????
	%- calibration (?) explicar en que consiste
	The Accomodation-Vergence conflict is a topic of great interest in the research field, therefore, a wide variety of  solutions have been proposed to try to solve this problem. 
	
	One of the many solutions found to ease this problem is applying blur to zones where the image not fuse correctly, this simulates the effect that the user would experience if the conflict would not have happened, see \cite{neareyeblur}. Related with this, a research \cite{sceneComposition} found that placing objects connecting different depth planes help the users to better transition between objects aiding to maintain the coupling between accommodation and vergence. Other line of research is to use eye-tracking techniques \cite{eyeTracking}, these technologies allow the system to know where the eye is pointing over the image and then blur the out of focus areas. 
	
	After all of this, we can see that this project takes a different approach from the main lines of research.  This project uses depth data collected from a stereo camera placed in front of the headset to change the distance between images inside the headset. However, to get the depth information a reliable and fast stereo matcher was needed. That is why ELAS \footnote{Acronym from Efficient large-scale stereo matcher.}\cite{LIBELAS} and its implementation LIBELAS were used.
		
	In the stereo matcher field, two main branches can be found, local stereo matchers and global stereo matchers. Global stereo matchers are reliable but use many compute power. On the contrary, local stereo matchers are fast but less reliable than the global matchers. ELAS uses first a global stereo matcher on highly reliable points and after removing the more redundant, a Delaunay triangle mesh is created from that points. After that, the regions are then processed by a local stereo matcher. More information about ELAS implementation can be found on \cite{LIBELAS}. 
	
	Using LIBELAS thought, introduces a new requirement, libelas requires that the input image pair must be rectified before using it. this is because libelas uses the epipolar lines\footnote{In stereo systems, a geometry relation between points from one image to the other. }, to find the disparities between images, this is done because is faster than searching over the hole image. LIBELAS assumes that these lines are in the same Y axis in both images, for that reason, a calibration module to correct the image distortion, rotation and translation is needed. To solve this issue, OpenCV calibration functions were selected.

	\section{Objectives}
	After the previous analysis and explanation of the problem, the main objectives of this project are the following: 

	\begin{itemize}
		\item Evaluate the user experience when using the HMD and determine whether the accommodation-vergence effect causes dizziness and general discomfort on the users. 
		
		\item Evaluate whether the user perceives incorrectly the distances and sizes of the objects when using a video-see-through HMD and if this issue supposes a problem for the users.
		
		\item If the Accomodation-Vergence is one of the causes of the discomfort on the users, the problem will be solved using the deph information that can be obtained using stereo vision. 
		
		\item If the user perception of size and distance when using a HMD is an issue, solve it by adapting the visualization of the scene to the distance of the objects in it.
		
		\item Evaluate the user experience after the development and conclude if our approach for solving these problems have reduced the discomfort of the users. 
		
		\item Add the required modules keeping in mind the usefulness of these for future developments.
		
		\item Add all the required modules in the visualization program without reducing the performance. 
	\end{itemize}

	\section{Methodology}
	%dudas sobre este apartado
	%hablar de tecnicas de desarrollo agil y sobretodo de las librerias utilizadas y de las limitaciones y ventajas que estas nos han aportado.
	
	Scrum and its variants are one of the most spread work methodologies nowadays. Therefore we believe convenient to use it as one of the foundation of this project, however, as this project was only done by one person, some changes were made. 
	
	As there was only one developer, the daily meeting was replaced with a weekly meeting with the project supervisors, in this case the tutor and the boss of the laboratory department in the CVC. In these meetings we evaluated the development done, the issues faced that week and the problems solved. Related with the scrum methodology, Github \cite{web:github},\cite{web:githubDesktop} was used as version control system and trello\cite{web:trello} as the task manager system.
	
	Involving the tools used to develop this project, C++ and QT \cite{web:qt} were used mainly because the previous development of the visualization system was done using that environment. QT libraries were used to develop the interface and the visualization of the images on the screen. In addition to that, OpenCV was used as the library for image processing and calibration, \cite{web:opencv}. Some code snippets for testing and evaluation of the results were done using Matlab\cite{web:matlab}. 
	
	%%%%%%%%
	%The program have the following pipeline.
	
	%First, the images are captured by the cameras placed in the front of the headset. Two independent threads will be grabbing these images and storing them in a temporal variable. This variable will have always the latest image captured.
	
	%Second, a thread is set to continuously be getting the latest image from the grabbing threads and displaying the region of interest (ROI) set in the configuration on the screens of the HMD. In addition to that, the images grabbed will be passed to a processing thread. 
	
	%Third, the processing thread first uses the already calculated calibration of the stereo to 
	
	
	%. After that, this thread will set these images on the processing thread
	
	%Parallel to this, the user settings will be loaded with the configuration of the last session. This configuration includes for each one of the distance settings the following parameters: distance between images and the region of interest over the images. The user can change and save these settings in any moment using the keyboard.
	%%%%%%
	
	
	
	\section{Tools and Development}
	
	First the tools and modules developed will be presented and after the pipeline and integration of the parts will be explained. 
	
	\subsection{Calibration}
	\label{sec:calib}
	%en este apartado hablare sobre que tecnicas se han usado para realizar la calibracion y como se ha implementado y %estructurado, explicar problemas encontrados para la calibracion (descalibracion constante parametros utilizados)
	
	The calibration module is key inside the pipeline to be able to get the LIBELAS depth map. This module can be split in several parts:  
	
	First, a dataset of captures of a chess like pattern have to be taken to get references of the distortion, rotation and distance of the cameras. In these images the pattern has to be visible by both cameras and cannot be occluded neither be partially out of the image. In addition to that, the dataset must contain images of the pattern in various positions, the more locations covered by the images the better the results of the calibration will be. 
	
	Second, once the dataset is taken, the next step is locating the chess pattern inside each image, to locate these points a OpenCV function is used first to find them and after to make these points more precise. In this step each camera dataset will be processed independently. 
	
	Once the points are calculated, they are passed to the monocular calibration function of OpenCV to obtain the intrinsic matrix and the distortion vector.  Although these parameters per se can be used in a rectification function to undistort the images, we need a stereo rectification that also rectifies the traslation and the rotation between cameras leaving the epipolar lines aligned. For that reason, a second phase of calibration is needed. 
	
	After the monocular calibration, these matrices along with the image points are passed to the stereo calibration function and the resulting matrices passed again to the rectification function. Once this function is finished we now have the projection and rotation matrices from both cameras, these can be passed to the undistortion function along with two images to rectify the distortion and align both images in the same epipolar lines, alternatively, these matrices can be passed to a undistort mapping function for a faster undistortion of the images.  
	
	Some problems were faced during the development of this module. The main issue was that some adjustment had to be done in the configuration parameters of the OpenCV function because excessive undistortion was done over the images leaving them completely unusable. The parameters adjusted were the number of coefficients from the distortion vector used by the calibration modules. 
	
	All of these matrices and configurations can be saved and loaded to avoid the need to calibrate each time the program is started.  
	
	\subsection{Libelas}
	 This module starts with two images that have already been undistorted with the stereo calibration, these then are passed to the LIBELAS processing function, this function computes the disparity from two given images and returns two disparity maps, one for each given image. These maps indicate us the distance between one pixel from one image to the other.  
	 
	 One of the problems faced by this library, and in general by depth map stereo matchers, is the existence of low textured areas on the image, as will be seen in \ref{sec:roiresize}, these regions usually end up without disparity values, therefore, potentially producing issues in further processing. These regions along with occluded ones and calibration faulty areas are usually set to negative values by Libelas itself, therefore can be easily detected.  
	 
	 These maps can also be visualized in grayscale by changing the range of the image or with a colormap to better appreciate the depth gradient. 
	 
	 One issue faced during the development of this module was the extremely high calibration precision needed in order to get quality images. The cameras, not totally locked in position, usually moved slightly, reducing the quality of the depth map obtained. The issue was communicated to the hardware development team in order to change the HDM prototype to better secure the cameras position.  
	 
	 Although LIBELAS is a fast library it was not quick enough to process the images in real time or near real time, to solve this issue, two possible solutions were found. 
	 
	 \begin{itemize}
	 	\item The first idea was to apply a ROI in the center of the images after the distortion, that way as the image is smaller, it requires less computing power to be processed. As the image is only cropped it will keep all the details of the center of the image, thats where usually the user will be looking at.  
	 	 
	 	\item Other idea was to make down sampling of the images before using them on LIBELAS. This also decreases the computing power required to process the images, however it also decreases the detail of the image, potentially affecting the quality of the depth map.  
 	 \end{itemize}
	 In \ref{sec:roiresize} a comparison between both methods can be seen. 
	 
	 To improve the quality of the depth map obtained in LIBELAS the default configuration called "Robotics" was taken as base and modified. First, we added the configuration that enables support points to be in the corners. We also removed the postprocessing features like median filtering or mean filtering. These last changes improve the speed without affecting the performance of the classification, as will be seen in \ref{pipeline} we already make operations over the image to calculate the classification values.  
	 
	\subsection{Configuration}
	
	Users have physically distinct vision systems between them, for example different eye sizes, different distance between eyes, etc. All of this influence the configuration that must be done to have a pleasant user experience. Also users should be able to adjust each configuration to the distance of the viewed objects.  For these reasons the following modules were developed.
	
	First, a module able to modify the position of the images over the screens of the HMD was developed. This module is designed to show only a ROI of the full resolution output of the camera, letting the user decide and change every possible aspect of this ROI, from the size of the ROI to the anchor point. These parameters can also be saved to be used in other sessions or to be able to change between different settings for different distances. Once the ROI is obtained graphic functions from QT are called to display them on the screen.
	
	Second, a module capable of make transitions between settings was developed. This module calculates the distance between two settings and does the transition in a given number of steps. It transitions the movement in two phases, first adjusting the zooming and then changing the position of ROI.
	
	\subsection{Pipeline and module integration}
	\label{sec:pipeline}
	%imagen del diseño del pipeline final, explicar clasificador 
	%presets, movimiento de las pantallas , guardado de parametros, integracion de las partes, threads
	%en este apartado se explicaran mejoras secundarias en el visor, creacion de los presets, smooth presets transition, movimiento de la roi por encima de las imagenes etc, y de como se han integrado todos los modulos dentro de visor intentando preservar en todo momento el rendimiento (threads, explicacion del pipeline final)
	
	Parallel to the development of said modules, a pipeline integrating them was built. This pipeline has the goal of combining the developed module to achieve a real time dynamic system that changes between already set configurations to adapt to the scene variations. This pipeline had to be fast and have quick response times, for that reason it was decided to use a threaded architecture to allow each thread to perform their tasks at their own pace without harming the performance of the thread in charge of updating the screen. This helps to reduce the reaction time improving the user experience. 
	
	A simple classifier has been develop to determine which distance the user is currently viewing. It uses the output of LIBELAS to perform a operation (mean, max or min) over the disparity map. The output value is then used on a threshold classifier to select between 3 different distances, near, medium and far. 
	To assure stability on the output of the classifier the output of the last frames are saved and then a mean is done between the current value and the previous ones. The ouput of this  is used as the input for the threshold classification. 
	
	As can be seen in Fig.\ref{fig:pipeline} the pipeline can be split in the following parts: 
	
	\begin{itemize}
		\item Grabbing threads: these threads grab images from the cameras that are set to be continuously capturing frames. After that, the images are then stored on a variable overwriting the previous image. This thread is continuously looping to grab the latest image. 
		
		\item Processing threads: this threat uses a image pair placed by the displaying thread to classify the image distance. This images will be first undistorted and then passed to LIBELAS to obtain the depth map. After that the central region is crop and the image is passed to the classifier. Then, the output classified distance is stored on a variable which will be queried each iteration by the displaying thread.  
		
		\item Displaying thread: the goal of this thread is to get the images from the grabbing threads and display them on the screen of the HMD. For this it uses the ROI module in combination with the setting system. After the ROI is done, the image is set on the QT graphic scene. QT then rescales the image to fit it on the screen of the device. This is used as a zoom since if a smaller region of the image is cropped it will be rescaled and the objects would look bigger. This thread also passes the images grabbed to the image processing thread, and uses its output, the classified distance, to dynamically select between user settings. Also, if set, it will use the transition module to smooth the change between settings. 
		
	\end{itemize}
	
	\begin{figure}
		\centering
		\includegraphics[width=1\linewidth]{img/pipeline.png}
		\caption{Diagram of the pipeline of the system. At top the grabbing threads that capture the images from the cameras. In the middle the thread that displays the images and changes between user settings. At the bottom the thread that process the images and classifies them.   }
		\label{fig:pipeline}
	\end{figure}
	
	
	\subsection{Offline pipeline}
	One of the issues had was that libelas was not able to produce real time processing, as is explained in \ref{}. For that reason, a video recording and processing modules were also developed.  
	
	The module was developed using OpenCV functions. This videos were intended to be saved in raw format, but due to an unresolved bug in the OpenCV modules that prevented the reading of raw videos, the videos were saved instead using the Intel's IYUV codec. %As it is lossless, the images are effectively the same as the images on the online pipeline.
	Other problem related with the recording module was that the videos have to be saved in memory, because storing them directly on disc is not possible because of the slow writing speeds of the drive will not be able to keep up with frame generation of the cameras, therefore not storing all the frames. Despite that, another issue was faced because the size of the in-memory stored images would rapidly increase overpassing the size limit imposed to 32 bits software. For this reason the whole project was modified to be compatible with 64 bits architectures.
	
	After the implementation of this module an offline pipeline was developed to allow the processing of these videos. The architecture is similar to the online pipeline seen in \ref{sec:pipeline}. thought has some noteworthy differences.  
	
	First, as it is not necessary this pipeline does not use a threaded architecture.
	
	Second, the images are read from a video instead of a camera, as the frames from the video can be grab on demand, every frame can be processed. The input video can be already undistorted or if that is not the case, the images are undistorted using the calibration module explained in \ref{sec:calib}. 
	
	Third, after the frame is processed by LIBELAS, the output depth map can be converted to a grayscale or colormap image and be saved in another video for further processing. Alternatively the images can be evaluated in place using the threshold classifier already explained.  
	
	%explicar analisis de los resultados usando matlab

	
	\section{Results}
	
	%\subsection{Calibration Comparison}
	%comparaciones entre imagenes no calibradas y imagenes calibradas
	%en este apartado compararemos las imagenes originales con calibracion obtenida via opencv y la obtenida via %matlab (quizas es un poco innceseraio este apartado ?)
	
	\subsection{Libelas}
	%rendimiento fps y resultados en cuanto a profundidad. imagenes!!
	asdasd
	\subsubsection{ROI vs resize}
	asdasd
	\label{sec:roiresize}
	
	
	\subsubsection{resize time performance}
	rendimiento tanto en tiempo como en classificacion segun el tamaño de la imagen escogida
	
	\subsection{Classsification}
	diferencias entre usar mean, max, min etc
	
	\subsection{performance stability}
	asdads

	
	
	\subsection{First user testing}
	
	In this user testing session, the goal was to take a preliminary evaluation on the Accommodation-Vergence conflict  and in the issue of the size of the objects. The protocol followed in this user testing session can be found in the appendix \ref{sec:annex:user1}.
	
	The results of this first user testing session can be seen in table \ref{tab:firstUserTestResults}, one main thing can be noticed, two of the six users that participated in the user testing sessions did only change slightly the parameters of the configuration. Meanwhile, the four other users changed the configuration. This is important because the users that did not change much the distance between image did not feel dizzy when using a setting in different distance than the original. On the contrary, the users that did change the distance between images mostly felt dizzy when using a setting set for a different distance than they were seeing.
	
	At the same time, it was noticed that in most cases, both types of users felt that the objects were further away when using the close setting in large distances, and vice versa in case of far settings in closer distances. This confirms a solution is needed to reduce this issue. 
	
	After this user testing session, it was agreed that a new user testing session have to take place to evaluate and resolve some of the problems faced in this one. 

	First we need to ensure that the users like S1 and S3 are correctly seeing the stereoscopic 3D and if this results repeat in different testing conditions. It is possible that the default setting was similar to they comfortable setting and therefore they did not think relevant to change it. For that reason in further user testing sessions a the default setting will be set to have a huge displacement on the X axis, forcing the users to always move their setting to the better fitting for them.
	
	Second, some users found difficult to set the vergence in the close distance for lack of a relevant 3D object or form, for this reason in further user testing an object could be introduced into the test to ease the setting up. 
	
	Third, during the testing of the settings in other distances, testing also the think that  also in the distance where the setting was set up may be necessary to ensure that the user have set correctly that distance.
	
	Fourth, we notice that some users found difficult to find the keys used to move the images, for that reason we decided to move the keys that change the position of the right image from i,j,k and l to the arrow keys. 
	
	Fifth, after the testing was done we notice that the cameras were slightly out of focus and the brightness, contrast and color balance was not correct, as this can also impact in the user comfortability, for now on before every user testing session, the general camera configuration will be check.
	
	
	\begin{table*}
		\begin{center}
			\begin{tabular}{cccccccc}
				\toprule
				& & \multicolumn{2}{c}{Near} & \multicolumn{2}{c}{Medium} & \multicolumn{2}{c}{Far} \\ 
				Subject &Feelings & $\Delta X$ & $\Delta Y$ & $\Delta X$ & $\Delta Y$ & $\Delta X$ & $\Delta X$ \\ 
				\midrule
				S1&Confortable & 0 & 6 & 3 & 0 & -9 & 0 \\ 
				\midrule 
				S2&Disconfort & 3 & 0 & -129 & 0 & -57 & 30 \\ 
				\midrule
				S3&Confortable & 3 & 0 & 3 & 0 & 3 & 0 \\ 
				\midrule 
				S4&Disconfort & 3 & -3 & -33 & 12 & -36 & -3 \\ 
				\midrule
				S5&Disconfort & -12 & 0 & -66 & 0 & -42 & 0 \\ 
				\midrule
				S6&Disconfort & -24 & 0 & -39 & 5 & -45 & -6 \\ 
				\bottomrule
			\end{tabular} 
			\caption{Results of the configuration of the ROIs in the first user testing session. The second column represents the general feelings of the users when changing the settings. Note that $\Delta X$ means separation between the center of the image pair in the X axis and $\Delta Y$ means separation between centers in the Y axis.}
			\label{tab:firstUserTestResults}
		\end{center}
	\end{table*}
	
	
	\subsection{Second user testing}
	%en este apartado mostraremos los distintos resultados obtenidos en la segunda sesion de user testing , explicaremos los resultados y concluiremos si nuestra hipotesis es correcta y si la solucion desarrollada es suficiente para resolver este problema, en caso de que no, nos plantearemos cuales son o han sido los problemas que impiden que el usuario sienta una mejora al utilizar la vergencia dinamica.
	
	For this second user testing session the goal was to evaluate if the Accommodation-Vergence conflict and the issue of the size of the objects affect the all users and whether the dynamic setting changer solves this problem. After the evaluation of the latest user testing some considerations were concluded. These have been taken into account in this second user testing with the goal of improving the consistency of the results. 
	
	Following the considerations of the last user testing, the right image was totally shifted with a starting difference between center points in the x axis of +158 pixels, $\Delta X$. This should force the users to adjust the setting and not be comfortable with the default configuration. Also a small cardboard box was used instead of the opthalmologic pattern to aid the configuration of the near settings. More detail of the protocol can be found in the annex \ref{sec:annex:user2}.
	
	Note that in this user testing subjects S1 and S3 from the first user testing session also participated in this session. This would allow us to evaluate if they still feel comfortable without changing the parameters or if all was a misunderstanding and they also need to change the configuration to feel comfortable. 
	
	\subsubsection{Accomodation-Vergence settings}
	The first thing that can be noticed from Fig.\ref{fig:ut:2:deltax} is that every user more or less have changed the distance between eyes to feel more comfortable with the distance currently viewed. Even the subjects S1 and S3 did change the distances. From these results can be deduced that the larger the distance is, the greater the separation between the images will be, since negative values mean greater distance between images. 
	
	It is possible though that with more further away objects, the increase of the distance between images will reduce, arriving to cap where the separation is no further increased.
	
	This is because the disparity between the images received by the eyes decreases when the objects are moved further away. Further investigation can be done in this topic.
	
	Related with this, as was expected this difference is not seen in the Y, this is because the Accomodation-Vergence issue is related with the disparity in the X axis. 
	
	\subsubsection{User opinion of the settings }
	Looking now at the user experience when using a setting in a different distances, see \ref{fig:ut:2:deltax}. Can be notice that users feel more comfortable using a setting at the original distance which it was set in. Also is noticeable that only 58.3\% of the users felt comfortable with the distance when using the distant setting in a distant environment, this also happens with the medium setting. On the contrary 83\% of the users said that they felt comfortable then using near configuration in near environments. This difference can be related with the fact that users in the near set up can interact with the objects therefore better fitting that configuration. 
	
	As the medium setting has the more intermediate values, is almost usable on every distance, thought this does not mean that would be comfortable to use it. More than half the users do not feel comfortable when using this medium setting on far distances. As expected, far and near configurations perform worse when used in the other's distance.
	
	After these results can be concluded that a module capable of changing the setting will be convenient to improve the user experience. 
	
	\subsubsection{Size issue}
	
	In Fig.\ref{fig:ut:2:size} a can be seen a comparison between the mean size of the ROI in every configuration. With these results it is clear that each distance needs a different size setting.  The users set smaller ROIs, bigger zooms over the image, to distant objects, seeing that objects bigger. On the contrary the users set bigger ROIs, smaller zooms, to closer objects, seeing them smaller. This results follow the expected pattern. A system capable of automatically changing the between settings would solve this problem. 
	
	\subsubsection{User opinion on the dynamic settings}
	As the pipeline was already finished, this user testing session also included a test of the system with the full pipeline working, capable of changing between presets. This module was used after the user had already set every configuration for each distance.  
	For the user testing, the mean of the central region of the 10 last images were saved to preserve stability in the output of the classification. None of the users complained about undesired changes of the settings when using this system.  
	
	First of all, all the users agreed that changing the parameters dynamically is better than having one setting for every distance and it is more convenient than having to change the setting manually. Also all agreed that after changing from one setting to the other no adaptation period is needed or it is short and not annoying.  
	
	In table \ref{tab:ut:2:dynamic}, can be seen that a tight majority of the users prefer not having a transition period when changing dynamically from one setting to the other. This can be related with the users also saying that this transition was slow and that the two differentiable phases of transition is not comfortable. In the other hand some users said that not using transitions was too abrupt even though it does not induced dizziness. 
	
	Can be concluded that the usage of the dynamic setting changer improves the user experience by giving the user the most suitable configuration for each distance. But it is also clear that the transition has improvement margin as most of the users agreed that it was slow and not comfortable to see.
	
	\begin{table*}
		\centering
		\begin{tabular}{@{}lccccccc@{}}
			\toprule
			\multicolumn{2}{l}{\multirow{2}{*}{\% positive feelings}} & \multicolumn{2}{c}{Near distance} & \multicolumn{2}{c}{Medium distance} & \multicolumn{2}{c}{Far distance} \\
			\multicolumn{2}{l}{} & Fused & Size & Fused & Size & Fused & Size \\ \midrule
			\multirow{3}{*}{Setting} & Near & 91.7 & 83.3 & 33.3 & 50.0 & 0.0 & 8.3 \\
			& Medium & 91.7 & 50.0 & 83.3 & 58.3 & 45.5 & 33.3 \\
			& Far & 66.7 & 0.0 & 58.3 & 33.3 & 91.7 & 58.3 \\ \bottomrule
		\end{tabular}
		\caption{Results in the second user testing session of the feelings of the users in percentage of positive responses. At top the distance where the testing was done and at the left the configuration used when that response was given. Fused means if the user are seeing the images correctly fused, and Size if the users are seeing the object's size similar to the reality.}
		\label{tab:ut:2:feelings}
	\end{table*}

	\begin{table}
		\centering
		\begin{tabular}{@{}ccc@{}}
			\toprule
			& Dynamic & \begin{tabular}[c]{@{}c@{}}Dynamic +\\ Transition\end{tabular} \\ \midrule
			Preferences in \% & 58.3    & 41.7                                                           \\ \bottomrule
		\end{tabular}
		\caption{Preferences of the user when using the dynamic setting changer.}
		\label{tab:ut:2:dynamic}
	\end{table}

	\begin{figure}
		\centering
		\includegraphics[width=1\linewidth]{img/fancydelta.png}
		\caption{Mean distance between center points of the images on each configuration. Note that $\Delta X$ means separation between the center of the image pair in the X axis.}
		\label{fig:ut:2:deltax}
	\end{figure}

	\begin{figure}
		\centering
		\includegraphics[width=1\linewidth]{img/userTestingSizechart.png}
		\caption{Mean size of the images in each configuration for each distance. Note that a bigger size means a bigger ROI and therefore smaller objects.}
		\label{fig:ut:2:size}
	\end{figure}


	
	\section{Conclusions}
	After the tests and the subsequent analysis were done, it was concluded that users experience dizziness when using HMD, this is related first, with the Accomodation-Vergence conflict and second by the issue with the size of the objects.
	Users have shown preference over the distance configuration that corresponded in each case to the distance they were seeing. This fits with the difference seen on the parameters of the configuration, where the users set the distance between images bigger when seeing distant objects and lower when looking at closer objects. 
	
	Also, was noticed that users set the zooming level depending on the distance they are seeing. For closer environments, the image is zoom out, meaning that objects are seen with smaller sizes. The contrary happens on objects seen in larger distances. User opinions during the testing also pointed that setting was needed to better fit each distance. All of this reinforced the idea that a system capable of changing between settings was necessary. Despite some issues with the calibration and the performance of the depth map generation, the dynamic changer was successfully developed using the LIBELAS library. All the users that participated in the user testing sessions agreed that the experience was better when using the dynamic setting changer. 
	
	Related with the general performance, as the modules were build using a threaded architecture, the high computing tasks like generating the depth map or making the rectification of the images did not compromise the experience of the viewer.
	
	
	
	
	
	\section{Future work}
	This projct is can be used as the foundation for many further research in the video-see-through HMD topic: 
	
	\begin{itemize}
		\item The depth map of this project can be to generate augmented reality objects over the real-world.
		
		\item As was seen in \ref{sec:art}, many researchers apply blur on the out of focus area of the image, to ease the Accomodation-Vergence conflict. Using the developed tools, we could go one step further using the depth map to apply this blur in objects on out of focus planes or applying it with more or less intensity depending on the depth. 
		
		\item This project can also be used to include more information on the environment by including a third camera with a different image spectrum.  
		
		\item Optimization of the software can be done and would allow to use it on smaller and less powerful devices. 
		
		\item Related with the depth map, it would be interesting to compare between the latest depth map generators and see their time performance. 
	\end{itemize}

	It is certain that this project has had some constrains that can be used as starting point for some research. These are mainly centered on the performance side. Improving the framerate and the image quality of the cameras is a thing that will happened over the years and will benefit hugely the video-see-through headsets improving the user experience. At the same time this would also improve the quality of the depth maps, with less errors and more detail. Also, an improvement in the general performance thanks to the increased computing power will benefit this system, allowing for faster generation of the depth map.  
	
	\section*{Acknowledgment}
	I want to thank, first of all, my tutor Felipe Lumbreras and the boss of the laboratory department Coen Atens, for their support and for sharing their knowledge. I also thank the users that participated in the user testing sessions. Thanks to the CVC and all their employees and researchers for providing the tools, the support and for letting me work in such inspiring place. And finally, I want to thank my friends and my family for their ideas and support this latest months. 
	
	This work was supported in part by a CVC transfer project with ProCare Light company, and partially funded by the Spanish Ministry of Economy and Competitiveness and FEDER under grants TIN2014-56919-C3-2-R and TIN2017-89723-P.
	
	\bibliography{biblio}
	\bibliographystyle{plain}
	
	\appendix
	
	\section{Objective and tasks list}
	lista de tareas y objetivos (similar a lo que tenia en las otras entregas)
	
	\section{User testing protocol}
	\subsection{First user testing protocol}
	\label{sec:annex:user1}
	The first user testing protocol used a ophthalmologic \ref{fig:add:pattern} pattern as reference for each one of the distances. The user testing followed this protocol: 
	
	\begin{itemize}
		\item All the default configuration should have the same parameters that and these will be reset for each user. This default configuration have both ROIs centered with a size that fills all the screen. 
		
		\item 3 patterns will be set at 3 different positions, reading distance (close distance), desktop pc screen distance (medium distance) and a wall a couple of meters away (far distance).
		
		\item Every user will start by adjusting a setting for each distance. 
		
		\item After the user has adjusted a setting for each distance, each of the configurations will be used in distances for which it was not configured. Then the user will be asked for each configuration to explain their feelings about the fusion of the images and if the size of them is similar to the reality.  
		
		\item The opinions of the users will be saved in a spreadsheet to use them on the analysis. 
		
		\item All the configuration files will be saved to use them on the analysis. 
	\end{itemize}
	
	
	 
	\subsection{Second user testing protocol}
	\label{sec:annex:user2}
	
	As a result of the considerations taken from the first user testing, some points of the first protocol \ref{sec:annex:user1} were modified:
	\begin{itemize}
		\item First of all the camera focus and brightness and color balance will be checked to ensure that the user testing session develops using optimal camera parameters.
		
		\item Instead of having the images centered, one image, the right one, will be set to have a great displacement to force the users to move and fit the distance between images. 
		
		\item Instead of having 3 patterns for each distance, in the case of the closer distance, an object, in this case a small cardboard box , will be used as the reference for the users to set their parameters. 
		
		\item After the user has adjusted a setting for each distance, each configuration will be tested on every position. The user will be asked for each configuration to explain their feelings about the fusion of the images and if the size of them is similar to the reality. 
		
		\item At the end of the user testing, the dynamic setting changer will be activated and the user will be asked about its feelings. After that the transition module will be activated and the user will be asked if this module improves the user experience over not using it.  
	
	\end{itemize}

	The points not mentioned remain the same as the protocol of the first user testing.  
	
	\section{Additional images}
	%En este apartado se incluiran imagenes extra de ejemplo(mas escenarios) i/o imagenes que no quepan en el documento en si
	
	poner imagen de linias epipolares.
	
	\begin{figure}
		\centering
		\includegraphics[width=1\linewidth]{img/Snellenchart_es-1.png}
		\caption{Ophthalmological pattern used in the user testing sessions. The pattern was downloaded from \cite{web:snellen}.}
		\label{fig:add:pattern}
	\end{figure}
	
	

\end{document}

